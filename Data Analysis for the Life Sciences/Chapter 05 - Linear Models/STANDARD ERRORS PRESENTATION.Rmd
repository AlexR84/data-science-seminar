---
title: "Standard Errors"
author: "Kezhen Fei"
date: "April 27, 2016"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##Falling object

-Where dose randomness come from?


-In our falling object example, randomness was introduced through measurement errors. 


-The gravitational constant is a fixed constant


-But, our estimate of the constant will change every time we perform the experiment. 


##Monte Carlo simulation of falling object

```{r}
set.seed(1)
B <- 10000
h0 <- 56.67
v0 <- 0
g <- 9.8 ##meters per second
n <- 25
tt <- seq(0,3.4,len=n) ##time in secs, t is a base function
X <-cbind(1,tt,tt^2)
##create X'X^-1 X'
A <- solve(crossprod(X)) %*% t(X)
betahat<-replicate(B,{
  y <- h0 + v0*tt  - 0.5*g*tt^2 + rnorm(n,sd=1)
  betahats <- A%*%y
  return(betahats[3])
})
head(betahat)

```

##Plot the estimated betahat:

```{r, echo=FALSE, message=FALSE}
library(rafalib)
mypar(1,2)
hist(betahat)
qqnorm(betahat)
qqline(betahat)
```
 
##Father and son heights example

-Given the random sample of father and son heights, assume we have the entire population

-Take a sample size of 50 over and over again

-Create 1000 samples of 50 pairs of father and son's heights

-Run linear model using father's height to predict son's height

-We'll get two estimates, intercept and beta1
```{r, echo=FALSE, message=FALSE}
library(UsingR)
x<-father.son$fheight
y<-father.son$sheight
n<-length(y)
N <- 50 
B <-1000
betahat <-replicate(B,{
  index <- sample(n,N)
  sampledat <- father.son[index,]
  x <-sampledat$fheight
  y <-sampledat$sheight
  lm(y~x)$coef
})
betahat <- t(betahat)#have estimates in two columns
```

## QQ-plot of intercept and beta1: Random Normal: 
```{r, echo=FALSE}
mypar(1,2)
qqnorm(betahat[,1])
qqline(betahat[,1])
qqnorm(betahat[,2])
qqline(betahat[,2])
```

##Correlation and Covariance
-The correlation of the two estimates
```{r}
cor(betahat[,1], betahat[,2])
```
-The covariance of two random variables is defined as:

```{r}
mean( (betahat[,1]-mean(betahat[,1] ))* (betahat[,2]-mean(betahat[,2])))
```

##Covariance is the correlation multiplied by the standard deviations of each random variable:

$$\mbox{Corr}(X,Y) = \frac{\mbox{Cov}(X,Y)}{\sigma_X \sigma_Y}$$

-It is a very useful quantity for mathematical derivations. 

-Use matrix algebra calculations to estimate standard errors of linear model estimates.

##Variance-covariance matrix $\boldsymbol{\Sigma}$
For a vector of variables $\mathbf{Y}$, $\boldsymbol{\Sigma}$ is defined as the matrix with the $i,j$ entry:

$$ \Sigma_{i,j} \equiv \mbox{Cov}(Y_i, Y_j) $$

For individual observations $Y_i$ sampled from a population, we assume independence of each observation and the $Y_i$ all have the same variance $\sigma^2$, so the $\boldsymbol{\Sigma}$ has two kinds of elements:

$$ \mbox{Cov}(Y_i, Y_i) = \mbox{var}(Y_i) = \sigma^2, \mbox{ for } i = j$$

$$ \mbox{Cov}(Y_i, Y_j) = 0, \mbox{ for } i \neq j$$

This implies that $\boldsymbol{\Sigma} = \sigma^2 \mathbf{I}$ with $\mathbf{I}$, the identity matrix (also called unity matrix).

## Variance of a linear combination 

The $\boldsymbol{\Sigma}$ of a linear combination $\mathbf{AY}$ of $\mathbf{Y}$ can be computed as:

$$
\mbox{var}(\mathbf{AY}) = \mathbf{A}\mbox{var}(\mathbf{Y}) \mathbf{A}^\top 
$$

For example, if $Y_1$ and $Y_2$ are independent both with variance $\sigma^2$ then:

$$\mbox{var}\{Y_1+Y_2\} = 
\mbox{var}\left\{ \begin{pmatrix}1&1\end{pmatrix}\begin{pmatrix} Y_1\\Y_2\\ \end{pmatrix}\right\}$$

$$ =\begin{pmatrix}1&1\end{pmatrix} \sigma^2 \mathbf{I}\begin{pmatrix} 1\\1\\ \end{pmatrix}=2\sigma^2$$

This result can be used to obtain the standard errors of the LSE (least squares estimate).

## LSE standard errors (Advanced)

Since $\boldsymbol{\hat{\beta}}$ is a linear combination of $\mathbf{Y}$: $\mathbf{AY}$ with $\mathbf{A}=\mathbf{(X^\top X)^{-1}X}^\top$, so we can use the equation above to derive the variance of our estimates:

$$\mbox{var}(\boldsymbol{\hat{\beta}}) = \mbox{var}( \mathbf{(X^\top X)^{-1}X^\top Y} ) =  $$

$$\mathbf{(X^\top X)^{-1} X^\top} \mbox{var}(Y) (\mathbf{(X^\top X)^{-1} X^\top})^\top = $$

$$\mathbf{(X^\top X)^{-1} X^\top} \sigma^2 \mathbf{I} (\mathbf{(X^\top X)^{-1} X^\top})^\top = $$

$$\sigma^2 \mathbf{(X^\top X)^{-1} X^\top}\mathbf{X} \mathbf{(X^\top X)^{-1}} = $$

$$\sigma^2\mathbf{(X^\top X)^{-1}}$$

The diagonal of the square root of this matrix contains the standard error of our estimates. 

## Estimating $\sigma^2$ 

Since the sample standard deviation of $Y$ is not $s$ because $Y$ also includes variability introduced by the deterministic part of the model: $\mathbf{X}\boldsymbol{\beta}$. The approach we take is to use the residuals. 

$$
\mathbf{r}\equiv\boldsymbol{\hat{\varepsilon}} = \mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}$$

Both $\mathbf{r}$ and $\boldsymbol{\hat{\varepsilon}}$ notations are used to denote residuals.

Then we use these to estimate, in a similar way, to what we do in the univariate case:

$$ s^2 \equiv \hat{\sigma}^2 = \frac{1}{N-p}\mathbf{r}^\top\mathbf{r} = \frac{1}{N-p}\sum_{i=1}^N r_i^2$$

Here $N$ is the sample size and $p$ is the number of parameters (including $\beta_0$). This will give us a better (unbiased) estimate.

## Father and Son Heights example
```{r}
n <- nrow(father.son)
N <- 50
index <- sample(n,N)
sampledat <- father.son[index,]
x <- sampledat$fheight
y <- sampledat$sheight
X <- model.matrix(~x)

N <- nrow(X)
p <- ncol(X)

XtXinv <- solve(crossprod(X))

resid <- y - X %*% XtXinv %*% crossprod(X,y)

s <- sqrt( sum(resid^2)/(N-p))
ses <- sqrt(diag(XtXinv))*s 
```

##Father & Son Height example (continued)
Let's compare to what `lm` provides:

```{r}
summary(lm(y~x))$coef[,2]
ses
```

## Linear combination of estimates

Frequently, we want to compute the standard deviation of a linear combination of estimates such as $\hat{\beta}_2 - \hat{\beta}_1$. This is a linear combination of $\hat{\boldsymbol{\beta}}$:

$$\hat{\beta}_2 - \hat{\beta}_1 = 
\begin{pmatrix}0&-1&1&0&\dots&0\end{pmatrix} \begin{pmatrix}
\hat{\beta}_0\\
\hat{\beta}_1 \\ 
\hat{\beta}_2 \\ 
\vdots\\
\hat{\beta}_p
\end{pmatrix}$$

Using the above, we know how to compute the variance covariance matrix of $\hat{\boldsymbol{\beta}}$.

## CLT and t-distribution

We have shown how we can obtain standard errors for our estimates. However, as we learned in the first chapter, to perform inference we need to know the distribution of these random variables. The reason we went through the effort to compute the standard errors is because the CLT applies in linear models. If $N$ is large enough, then the LSE will be normally distributed with mean $\boldsymbol{\beta}$ and standard errors as described. For small samples, if the $\varepsilon$ are normally distributed, then the $\hat{\beta}-\beta$ follow a t-distribution. We do not derive this result here, but the results are extremely useful since it is how we construct p-values and confidence intervals in the context of linear models.

## Code versus math

The standard approach to writing linear models either assume the values in $\mathbf{X}$ are fixed or that we are conditioning on them. Thus  $\mathbf{X} \boldsymbol{\beta}$ has no variance as the $\mathbf{X}$ is considered fixed. This is why we write $\mbox{var}(Y_i) = \mbox{var}(\varepsilon_i)=\sigma^2$. This can cause confusion in practice. For example:

```{r}
x =  father.son$fheight
beta =  c(34,0.5)
var(beta[1]+beta[2]*x)
```

it is nowhere near 0. The function `var` is simply computing the variance of the list we feed it, while the mathematical definition of variance is considering only quantities that are random variables. In the R code above, `x` is not fixed at all. 

## Code versus math (Continued)

Similarly, if we use R to compute the variance of $Y$ in our object dropping example, we obtain something very different than $\sigma^2=1$ (the known variance):

```{r}
n <- length(tt)
y <- h0 + v0*tt  - 0.5*g*tt^2 + rnorm(n,sd=1)
var(y)
```

Again, this is because we are not fixing `tt`. 

We need to be careful in distinguishing code from math!!