---
title: "Monte Carlo Simulation"
author: "Eleanor Howe"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: ioslides_presentation
---

---
layout: page
title: Monte Carlo methods
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

```{r,include=FALSE}
set.seed(1)
```


## Monte Carlo Simulation

Monte Carlo simulation is a method of imitating random variables from the real world. 

We use this method to look at the properties of variables using simulation, rather than theoretical distributions.

We can test whether an ideal (theoretical) distribution is a good fit for our data, given the sample size. 

As an example, let's use a Monte Carlo simulation to compare the Central Limit Theorem to the t-distribution approximation for different sample sizes.


```{r,message=FALSE,echo=FALSE}
library(rafalib)
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv"
filename <- "mice_pheno.csv"
if (!file.exists(filename)) download(url,destfile=filename)
```

```{r,message=FALSE, echo=F}
library(dplyr)
dat <- read.csv("mice_pheno.csv")
controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>%  
  select(Bodyweight) %>% unlist
```

## Selecting data

We will build a function that automatically generates a t-statistic under the null hypothesis for any sample size of `n`. We'll use the mouse phenotype data we've been working with recently (weight of femailes on a chow diet). 

```{r}
ttestgenerator <- function(n) {
  #note that here we have a false "high fat" group where we actually
  #sample from the chow or control population. 
  #This is because we are modeling the null.
  cases <- sample(controlPopulation,n)
  controls <- sample(controlPopulation,n)
  tstat <- (mean(cases)-mean(controls)) / 
      sqrt( var(cases)/n + var(controls)/n ) 
  return(tstat)
  }
ttests <- replicate(1000, ttestgenerator(10))
```

## The distribution

With 1,000 Monte Carlo simulated occurrences of this random variable, we can now get a glimpse of its distribution:

```{r ttest_hist, fig.cap="Histogram of 1000 Monte Carlo simulated t-statistics."}
hist(ttests)
```

## Q-Q Plot

That histogram looked like a pretty normal distribution. But is it? A Q-Q plot can tell us. 

Plot the quantiles of the simulated data from the function 'ttestgenerator' against the quantiles of an idealized distribution (normal).

## Q-Q Plot of sample size 10

```{r ttest_qqplot, fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics to theoretical normal distribution."}
qqnorm(ttests)
abline(0,1)
```

## Q-Q Plot of sample size 10

This looks like a very good approximation because the points fall on the identity line. 

For this particular population, a sample size of 10 was large enough to use the CLT approximation. 

How good an approximation is the CLT for sample sizes of 3? 

## Q-Q Plot of sample size 3

```{r, ttest_df3_qqplot,fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics with three degrees of freedom to theoretical normal distribution."}
ttests <- replicate(1000, ttestgenerator(3))
qqnorm(ttests)
abline(0,1)
```

## Q-Q Plot of sample size 3

Large quantiles (tails) are larger than expected. CLT is not a good choice. 

Let's have a look at a t-distribution instead.

<!--Now we see that the large quantiles, referred to by statisticians as
the _tails_, are larger than expected (below the line on the left side
of the plot and above the line on the right side of the plot).  In the
previous module, we explained that when the sample size is not large
enough and the *population values* follow a normal distribution, then
the t-distribution is a better approximation. Our simulation results
seem to confirm this:
-->

## Q-Q Plot of the t-distribution


```{r, ttest_v_tdist_qqplot,fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics with three degrees of freedom to theoretical t-distribution."}
ps <- (seq(0,999)+0.5)/1000
qqplot(qt(ps,df=2*3-2),ttests,xlim=c(-6,6),ylim=c(-6,6))
abline(0,1)
```

## Q-Q Plot of the t-distribution

In this case, a t-distribution is a better model than a simulated distribution. We learned before that when a sample size is not large enough and the *population values* follow a normal distribution, then the t-distribution is a better approximation than the CLT. 

However in this case the t-distribution is still not perfect. Why?

<!--The t-distribution is a much better approximation in this case, but it is still not perfect. This is due to the fact that the original data is not that well approximated by the normal distribution.
-->

## Q-Q Plot of the t-distribution
It turns out that the original data is not that well approximated by the normal distribution in the first place.
```{r, dat_qqplot, fig.cap="Quantile-quantile of original data compared to theoretical quantile distribution."}
qqnorm(controlPopulation, main="Original data vs. theoretical quantile distribution")
qqline(controlPopulation)
```

## Parametric Simulations

These previous exercises were educational only, because we had a complete population to look at and compare samples against. 

We normally don't have a full population to work with - only samples, and we must infer a population from them.

We use Monte Carlo methods to create a population from a theoretical distribution, using parameters derived from the real data (mean and standard deviation).
<!--
The technique we used to motivate random variables and the null
distribution was a type of Monte Carlo simulation. We had access to
population data and generated samples at random. In practice, we do
not have access to the entire population. The reason for using the
approach here was for educational purposes. However, when we want to
use Monte Carlo simulations in practice, it is much more typical to
assume a parametric distribution and generate a population from
this, which is called a _parametric simulation_. This means that we take
parameters estimated from the real data (here the mean and the standard
deviation), and plug these into a model (here the normal
distribution).  This is actually the most common form of Monte Carlo
simulation. 
-->

For this case of mouse weights, we could use our knowledge that mice typically weigh 24 grams with a SD of about 3.5 grams, and that the distribution is approximately normal, to generate population data.


## T-test generator for parametric simulation

After we generate the data, we can then repeat the exercise above. We no longer have to use the `sample` function since we can re-generate random normal numbers. The `ttestgenerator` function therefore can be written as follows: 

```{r}
controls<- rnorm(5000, mean=24, sd=3.5)
ttestgenerator <- function(n, mean=24, sd=3.5) {
  cases <- rnorm(n,mean,sd)
  controls <- rnorm(n,mean,sd)
  tstat <- (mean(cases)-mean(controls)) / 
      sqrt( var(cases)/n + var(controls)/n ) 
  return(tstat)
  }
```

## Parametric simulation sample size of 10

```{r parametricsimulation3}
ttests <- replicate(1000, ttestgenerator(10))
qqnorm(ttests, main="T-tests on simulated mouse data vs. normal distribution")
abline(0,1)
```


## Parametric simulation sample size of 3

```{r parametricsimulation10}
ttests <- replicate(1000, ttestgenerator(3))
qqnorm(ttests, main="T-tests on simulated mouse data vs. normal distribution")
abline(0,1)
```
