---
title: "Power Calculations (p. 62)"
author: |
  | Data Analysis for the Life Sciences
  | CUNY School of Public Health
  | Waldron Book Club
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontsize: 12pt
output: html_document
---

## Power Calculations 

#### Introduction

```{r}
suppressWarnings(library(downloader))
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv"
filename <- "mice_pheno.csv"
if(!file.exists(filename)) {
  download(url,destfile=filename)
}
```

We have used the example of the effects of two different diets on the weight of mice. Since in this illustrative example we have access to the population, we know that in fact there is a substantial (about 10%) difference between the average weights of the two populations:

```{r,message=FALSE}
library(dplyr)
dat <- read.csv("mice_pheno.csv") #Previously downloaded 

controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>%  
  select(Bodyweight) %>% unlist

hfPopulation <- filter(dat,Sex == "F" & Diet == "hf") %>%  
  select(Bodyweight) %>% unlist

mu_hf <- mean(hfPopulation)
mu_control <- mean(controlPopulation)
print(mu_hf - mu_control)
print((mu_hf - mu_control)/mu_control * 100) # percent increase
```

We've seen that when sample sizes are small, they don't always approximate the population parameter. In this case we take $N = 5$ and we get p-value that is not statistically significant. 

```{r}
set.seed(1)
N <- 5
hf <- sample(hfPopulation,N)
control <- sample(controlPopulation,N)
t.test(hf,control)$p.value
```

How do we determine the appropriate power to be able to detect a significant effect? 

#### Types of Error

_Plot from Behavioral Research Data Analysis with R by Yuelin Li and Jonathan Baron_

[ShinyAppLive](https://mramos.shinyapps.io/PowerCalc/)

|                    |                         The null hypothesis ($H_O$) is...                                |
|--------------------|:---------------------------------------:|:------------------------------------------------:|
| __Statistical Result__ | $H_0$ = TRUE                          | $H_0$ = FALSE                                    |
| Reject $H_0$       | $\alpha$ or **Type I Error**: falsely reject $H_0$ | Power (1-$\beta$: correctly reject $H_0$ |
| Accept $H_0$       | 1-$\alpha$: correctly accepting $H_0$ | $\beta$ or **Type II error**: falsely accept $H_0$    |

Suppose we have a sample size of 12 at an alpha level of 0.05. Now we can simulate 2000 times: 

```{r}
N <- 12
alpha <- 0.05
B <- 2000
```

```{r}
reject <- function(N, alpha=0.05){
   hf <- sample(hfPopulation,N) 
   control <- sample(controlPopulation,N)
   pval <- t.test(hf,control)$p.value
   pval < alpha
}
```

Run one simulation for a simple size of 12. Did we reject? 

```{r}
reject(12)
```

Repeat this $B$ times when $N=12$: 
We get a vector of logicals... 

```{r}
rejections <- replicate(B, reject(N))
```

We can calculate the proportion of times we correctly reject. So with $N = 12$, power is:

```{r}
mean(rejections)
```

Let's take a vector of possible values we want to pass on to our calculations: 

```{r}
Ns <- seq(5, 50, 5)
```

Using the "simple apply" `sapply` function, we can do it for all the values of N that we want to use. 

```{r}
power <- sapply(Ns, function(N) {
  rejections <- replicate(B, reject(N))
  mean(rejections)
})
```

We can plot the result like so: 

```{r}
plot(Ns, power, type = "b")
```

We can also vary the alpha level and trade off between the types of errors as showin in the shiny app. 

```{r}
N <- 30
alphas <- c(0.1,0.05,0.01,0.001,0.0001)
power <- sapply(alphas,function(alpha){
  rejections <- replicate(B,reject(N,alpha=alpha))
  mean(rejections)
})
options(scipen = 999)
plot(alphas, power, xlab="alpha", type="b", log="x")
```

#### p-values are arbitrary under $H_a$

When the alternative hypothesis is true, 
we can make a p-value as small as we want simply by increasing
the sample size (supposing that we have an infinite population to sample
from).

```{r}
calculatePvalue <- function(N) {
   hf <- sample(hfPopulation,N) 
   control <- sample(controlPopulation,N)
   t.test(hf,control)$p.value
}
```

We have a limit here of 200 for the high-fat diet population, but we can
see the effect well before we get to 200.
For each sample size, we will calculate a few p-values. We can do
this by repeating each value of $N$ a few times.

```{r}
Ns <- seq(10,200,by=10)
Ns_rep <- rep(Ns, each=10)
pvalues <- sapply(Ns_rep, calculatePvalue)
```

```{r}
plot(Ns_rep, pvalues, log="y", xlab="sample size",
     ylab="p-values")
abline(h=c(.01, .05), col="red", lwd=2)
text(x = 150, y = 0.065, labels = "0.05")
text(x = 150, y = 0.0125, labels = "0.01")
```

Tiny p-values are not more interesting. They just mean that more mice were sampled than necessary. Only a reasonable threshold is needed to reject the null hypothesis. A better statistic to report is the effect size with a confidence interval. 


```{r}
N <- 12
hf <- sample(hfPopulation, N)
control <- sample(controlPopulation, N)
diff <- mean(hf) - mean(control)
diff / mean(control) * 100
t.test(hf, control)$conf.int / mean(control) * 100
```

In addition, we can report a statistic called
[Cohen's d](https://en.wikipedia.org/wiki/Effect_size#Cohen.27s_d),
which is the difference between the groups divided by the pooled standard
deviation of the two groups. 

```{r}
sd_pool <- sqrt(((N-1)*var(hf) + (N-1)*var(control))/(2*N - 2))
diff / sd_pool
```

This tells us how many standard deviations of the data the mean of the
high-fat diet group is from the control group. Under the
alternative hypothesis, unlike the t-statistic which is guaranteed to
increase, the effect size and Cohen's d will become more precise.
